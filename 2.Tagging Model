{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.Tagging Model","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LNUyFFXe44Zl","colab_type":"code","outputId":"5b59f46b-9ff7-466c-94da-d76360cc96b2","executionInfo":{"status":"ok","timestamp":1571545489720,"user_tz":-480,"elapsed":23132,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DyNjpxfW6U4q","colab_type":"code","outputId":"8d2a6e1e-4c18-453b-c756-ce95fc80d5de","executionInfo":{"status":"ok","timestamp":1571545558366,"user_tz":-480,"elapsed":66731,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install allennlp"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n","Collecting ftfy (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 25.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.5)\n","Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 54.1MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.250)\n","Collecting flask-cors>=3.0.7 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n","Collecting parsimonious>=0.8.0 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 23.6MB/s \n","\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Collecting numpydoc>=0.8.0 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n","Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n","\u001b[K     |████████████████████████████████| 256kB 52.0MB/s \n","\u001b[?25hCollecting word2number>=1.1 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Collecting responses>=0.7 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n","Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.8)\n","Collecting tensorboardX>=1.2 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n","\u001b[K     |████████████████████████████████| 194kB 17.4MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n","Collecting flaky (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n","Collecting jsonpickle (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n","Collecting overrides (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/7a/b2/2cb6a3fc8ee1dc8617e07e476be19723748ddfcce0c6b9db7a5f2d5b9598/overrides-2.0.tar.gz\n","Collecting conllu==1.3.1 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n","Collecting pytorch-transformers==1.1.0 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 40.5MB/s \n","\u001b[?25hRequirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n","Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.0+cu100)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n","Collecting unidecode (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 60.4MB/s \n","\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.3)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->allennlp) (1.12.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.9.11)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n","Collecting regex (from pytorch-pretrained-bert>=0.6.0->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n","\u001b[K     |████████████████████████████████| 645kB 43.5MB/s \n","\u001b[?25hRequirement already satisfied: botocore<1.13.0,>=1.12.250 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.250)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.2.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.2.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n","Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.1.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n","Collecting sentencepiece (from pytorch-transformers==1.1.0->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 44.9MB/s \n","\u001b[?25hRequirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->allennlp) (0.15.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.2)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n","Building wheels for collected packages: ftfy, parsimonious, numpydoc, jsonnet, word2number, overrides\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=923b6a1caff013625480e67058551409aeb65a6ba3c53f8e9638594acef19758\n","  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=10ff8d924c4cfd2f4cc467749c37fb4bebbfdb230aad3cca9ec74a7f9e577a7e\n","  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n","  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for numpydoc: filename=numpydoc-0.9.1-cp36-none-any.whl size=31872 sha256=965048e12ab253e3fd7b3b0e8ae771a2cea85c0fbbdb677c660c8afa1286df44\n","  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320376 sha256=6f0d48253fcee62ca47e1ae2aefc6c7cb00b0baecce8d913babfa2665061a61c\n","  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=0e9dc47c2240abf02b2ac0f742839aba77d9fe0dfac134a687276fed86649889\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.0-cp36-none-any.whl size=4222 sha256=891c53194051aefb7273b04de51edf2b7a4182984556eb028c6ec80deb0454a4\n","  Stored in directory: /root/.cache/pip/wheels/67/ab/57/d68b6dad468ff96b792770a83229451add2b347b0c12a10300\n","Successfully built ftfy parsimonious numpydoc jsonnet word2number overrides\n","Installing collected packages: ftfy, regex, pytorch-pretrained-bert, flask-cors, parsimonious, numpydoc, jsonnet, word2number, responses, tensorboardX, flaky, jsonpickle, overrides, conllu, sentencepiece, pytorch-transformers, unidecode, allennlp\n","Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-2.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 regex-2019.8.19 responses-0.10.6 sentencepiece-0.1.83 tensorboardX-1.9 unidecode-1.1.1 word2number-1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ckJ_LjzCk-Q","colab_type":"code","outputId":"9107733a-b704-49e7-a6bd-50e7b2ae63c4","executionInfo":{"status":"ok","timestamp":1569847359058,"user_tz":-480,"elapsed":824,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["#used to double check if file is in the right form\n","'''\n","test_file = 'drive/My Drive/Colab Notebooks/dat/test.txt'\n","with open(test_file, \"r\") as f:\n","    for line_num, line_full in enumerate(f):\n","      if line_num <10:\n","        line_id,line = line_full.split(\":\",1)\n","        print(line_full.split(\":\",1))'''"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['10000', 'repair~B-Act slop~O pipe~B-Item positioner~I-Item arm~O cyl~O mou~O \\n']\n","['10001', 'check~B-Act pressure~B-Item and~O condition~O track~B-Item adjust~B-Act \\n']\n","['10002', 'right-hand~O track~B-Item not~B-Stat working~I-Stat \\n']\n","['10003', 'changeout~B-Item lower~I-Item stacker~O valve~O relief~O h~O \\n']\n","['10004', 'very~B-Stat high~O dust~O level~B-Item in~O op_cab~O \\n']\n","['10005', 'rod~B-Item fork~B-Item need~O build~B-Act up~O \\n']\n","['10006', 'thread~O spray~O need~O weld~B-Act \\n']\n","['10007', 'change~B-Act eng~B-Item air~I-Item filter~O once~O per~O shift~O \\n']\n","['10008', 'jump~B-Item start~I-Item machine~O \\n']\n","['10009', 'drill~O required~O jump~B-Item start~I-Item \\n']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g2Rzj0MAKfSr","colab_type":"code","colab":{}},"source":["#===========Parameters==============\n","bidirection =False\n","#===========Parameters=============="],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpSZKgcZqmpP","colab_type":"code","colab":{}},"source":["#adapted from the tutorial found at https://allennlp.org/tutorials\n","\n","from typing import Iterator, List, Dict\n","import torch\n","import sys\n","\n","from allennlp.data import Instance\n","from allennlp.data.fields import TextField, SequenceLabelField\n","from allennlp.data.dataset_readers import DatasetReader\n","\n","from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n","from allennlp.data.tokenizers import Token\n","from allennlp.data.vocabulary import Vocabulary\n","from allennlp.models import Model\n","from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n","from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n","from allennlp.training.metrics import CategoricalAccuracy\n","\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n","\n","torch.manual_seed(1)\n","\n","\n","class MaintenanceDatasetReader(DatasetReader):\n","    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n","        super().__init__(lazy=False)\n","        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n","\n","    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n","        sentence_field = TextField(tokens, self.token_indexers)\n","        fields = {\"sentence\": sentence_field}\n","\n","        if tags:\n","            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n","            fields[\"labels\"] = label_field\n","\n","        return Instance(fields)\n","\n","    def _read(self, file_path: str) -> Iterator[Instance]:  # read in the file\n","        with open(file_path) as f:\n","            for line in f:\n","                pairs = line.strip().split()\n","                # remove white space from start and end and then split words apart\n","                try:\n","                  sentence, tags = zip(*(pair.split(\"~\") for pair in pairs))\n","                except:\n","                  #if there are any errors, see where it failed\n","                  print('failed on: ' , pairs)\n","                  continue\n","                  #sys.exit(1)\n","                yield self.text_to_instance([Token(word) for word in sentence], tags)\n","\n","\n","class LstmTagger(Model):\n","    def __init__(self,\n","                 word_embeddings: TextFieldEmbedder,  # convert tokens into tensors\n","                 encoder: Seq2VecEncoder,\n","                 vocab: Vocabulary) -> None:\n","        super().__init__(vocab)\n","        self.word_embeddings = word_embeddings\n","        self.encoder = encoder\n","        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n","                                          out_features=vocab.get_vocab_size('labels'))\n","        self.accuracy = CategoricalAccuracy()\n","\n","    # where the main computation happens\n","    def forward(self,\n","                sentence: Dict[str, torch.Tensor],\n","                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n","        mask = get_text_field_mask(sentence)\n","        embeddings = self.word_embeddings(sentence)\n","        encoder_out = self.encoder(embeddings, mask)\n","        tag_logits = self.hidden2tag(encoder_out)\n","        output = {\"tag_logits\": tag_logits}\n","        if labels is not None:\n","            self.accuracy(tag_logits, labels, mask)\n","            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n","\n","        return output\n","\n","    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n","        return {\"accuracy\": self.accuracy.get_metric(reset)}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hHotjid2ZM0","colab_type":"code","outputId":"5d09cf22-4914-43ce-dbae-ba3f712ee63e","executionInfo":{"status":"ok","timestamp":1571556527310,"user_tz":-480,"elapsed":2410,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["#adapted from the tutorial found at https://allennlp.org/tutorials\n","\n","reader = MaintenanceDatasetReader()\n","\n","train_dataset = reader.read('drive/My Drive/Colab Notebooks/dat/train.txt')\n","validation_dataset = reader.read('drive/My Drive/Colab Notebooks/dat/validate.txt')\n","\n","vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 6\n","\n","'''\n","#Code to change to glove vectors\n","#the pre-trained GloVe file can be found from https://nlp.stanford.edu/projects/glove/ \n","#and should be placed in the same location as the other files\n","\n","token_embedding = Embedding.from_params(\n","                            vocab=vocab,\n","                            params=Params({'pretrained_file': 'glove.42B.300d.txt',\n","                                           'embedding_dim': EMBEDDING_DIM})\n","                            )\n","word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n","'''\n","\n","\n","token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n","                            embedding_dim=EMBEDDING_DIM)\n","word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n","\n","\n","lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n","                                           bidirectional=bidirection, batch_first=True))\n","model = LstmTagger(word_embeddings, lstm, vocab)\n","if torch.cuda.is_available():\n","    cuda_device = 0\n","    model = model.cuda(cuda_device)\n","else:\n","    cuda_device = -1\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["4000it [00:01, 3682.78it/s]\n","1000it [00:00, 2403.05it/s]\n","100%|██████████| 5000/5000 [00:00<00:00, 184932.41it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eFd2WDZO0ezM","colab_type":"code","outputId":"40f6a2a4-a007-4ca1-ea84-9febdbed69fa","executionInfo":{"status":"ok","timestamp":1571556602734,"user_tz":-480,"elapsed":71616,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#adapted from the tutorial found at https://allennlp.org/tutorials\n","\n","from typing import Iterator, List, Dict\n","import torch\n","import torch.optim as optim\n","from allennlp.data.iterators import BucketIterator\n","from allennlp.training.trainer import Trainer\n","import time\n","\n","\n","start = time.time()\n","\n","torch.manual_seed(1)\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","iterator = BucketIterator(batch_size=2, sorting_keys=[(\"sentence\", \"num_tokens\")])\n","iterator.index_with(vocab)\n","trainer = Trainer(model=model,\n","                  optimizer=optimizer,\n","                  iterator=iterator,\n","                  train_dataset=train_dataset,\n","                  validation_dataset=validation_dataset,\n","                  patience=5,  # stop training if loss doesnt improve after 10 epoch\n","                  num_epochs=1000,\n","                  cuda_device=cuda_device,)\n","trainer.train()\n","\n","end = time.time()\n","print(end - start)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.6736, loss: 0.9012 ||: 100%|██████████| 2000/2000 [00:08<00:00, 237.03it/s]\n","accuracy: 0.8322, loss: 0.4612 ||: 100%|██████████| 500/500 [00:00<00:00, 731.51it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.8692, loss: 0.3546 ||: 100%|██████████| 2000/2000 [00:08<00:00, 249.65it/s]\n","accuracy: 0.8710, loss: 0.3571 ||: 100%|██████████| 500/500 [00:00<00:00, 757.71it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.9050, loss: 0.2602 ||: 100%|██████████| 2000/2000 [00:08<00:00, 244.36it/s]\n","accuracy: 0.8746, loss: 0.3522 ||: 100%|██████████| 500/500 [00:00<00:00, 745.56it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.9199, loss: 0.2214 ||: 100%|██████████| 2000/2000 [00:08<00:00, 245.99it/s]\n","accuracy: 0.8772, loss: 0.3526 ||: 100%|██████████| 500/500 [00:00<00:00, 735.42it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.9290, loss: 0.1907 ||: 100%|██████████| 2000/2000 [00:08<00:00, 245.47it/s]\n","accuracy: 0.8728, loss: 0.3605 ||: 100%|██████████| 500/500 [00:00<00:00, 728.25it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.9382, loss: 0.1699 ||: 100%|██████████| 2000/2000 [00:08<00:00, 242.40it/s]\n","accuracy: 0.8710, loss: 0.3711 ||: 100%|██████████| 500/500 [00:00<00:00, 737.99it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.9413, loss: 0.1564 ||: 100%|██████████| 2000/2000 [00:08<00:00, 245.73it/s]\n","accuracy: 0.8730, loss: 0.3819 ||: 100%|██████████| 500/500 [00:00<00:00, 737.57it/s]\n","unable to check gpu_memory_mb(), continuing\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n","    encoding='utf-8')\n","  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n","    **kwargs).stdout\n","  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n","    output=stdout, stderr=stderr)\n","subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n","accuracy: 0.9454, loss: 0.1456 ||: 100%|██████████| 2000/2000 [00:08<00:00, 247.38it/s]\n","accuracy: 0.8721, loss: 0.4014 ||: 100%|██████████| 500/500 [00:00<00:00, 747.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["71.01065468788147\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uE6XL2g_19Wc","colab_type":"code","colab":{}},"source":["#change file name to differentiate between models\n","model_file = \"drive/My Drive/Colab Notebooks/dat/Models/LSTM_4000_1000_1hot_Uni.th\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCNtotVi1sh2","colab_type":"code","outputId":"a652fe0a-4283-43ad-f910-bbe970952197","executionInfo":{"status":"ok","timestamp":1571556704034,"user_tz":-480,"elapsed":2263,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#save model so it can be used for processing at any time\n","with open(model_file, 'wb') as f:\n","    torch.save(model.state_dict(), f)\n","vocab.save_to_files(\"drive/My Drive/Colab Notebooks/dat/vocabulary\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:root:vocabulary serialization directory drive/My Drive/Colab Notebooks/dat/vocabulary is not empty\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_uTbbH5M2f1a","colab_type":"code","outputId":"007037ff-078a-43bc-b721-129f1413e3d2","executionInfo":{"status":"ok","timestamp":1571556785141,"user_tz":-480,"elapsed":2577,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["import time\n","start = time.time()\n","import sys\n","import torch\n","import numpy as np\n","from allennlp.data.vocabulary import Vocabulary\n","from allennlp.predictors import SentenceTaggerPredictor\n","\n","import re\n","option = 1\n","test_file = 'drive/My Drive/Colab Notebooks/dat/ExpertTest.txt'\n","output_txt = 'drive/My Drive/Colab Notebooks/dat/Prediction.txt'\n","\n","\n","model = LstmTagger(word_embeddings, lstm, vocab)\n","with open(model_file, 'rb') as f:\n","    model.load_state_dict(torch.load(f))\n","if cuda_device > -1:\n","    model.cuda(cuda_device)\n","predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n","\n","if option == 1:\n","\n","    measurements = {'Act':      {'Act': 0, 'Item': 0, 'Stat': 0, 'O': 0, 'Total': 0},\n","                    'Item':     {'Act': 0, 'Item': 0, 'Stat': 0, 'O': 0, 'Total': 0},\n","                    'Stat':     {'Act': 0, 'Item': 0, 'Stat': 0, 'O': 0, 'Total': 0},\n","                    'O':        {'Act': 0, 'Item': 0, 'Stat': 0, 'O': 0, 'Total': 0},\n","                    'Total':    {'Act': 0, 'Item': 0, 'Stat': 0, 'O': 0, 'Total': 0}}\n","\n","    with open(test_file, \"r\") as f, open(output_txt, \"w\") as txt:\n","        for line_num, line_full in enumerate(f):\n","            line_id,line = line_full.split(\":\",1)\n","            if line == '' or line.startswith('+'):\n","                continue\n","            line = line.strip()  # remove the newline character\n","            text = ' '.join(re.split(r\"[~ ]\", line)[0::2])\n","            tags = (re.split(r\"[~ ]\",    line)[1::2])\n","            text = re.sub(r'([^\\s\\w]|_)+', '', text)\n","            if text == '':\n","                continue\n","            taggedData = predictor.predict(text)['tag_logits']\n","            tagsEst = np.argmax(taggedData, axis=-1)\n","\n","            txt.write(line_id + \":\"+ ' '.join([a + b + c for a, b, c in\n","                                zip(re.split(\" \", text), ['~'] * len(tagsEst),\n","                                    [model.vocab.get_token_from_index(i, 'labels') for i in tagsEst])]) + '\\n')\n","            \n","            # sanity check, make sure the number of tags is the same\n","            if len(tagsEst) == len(tags):\n","                for i, tag in enumerate(tags):\n","                    # if tags match, increment accuracy\n","                    tag_real = tags[i]\n","                    tag_pred = model.vocab.get_token_from_index(tagsEst[i], 'labels')\n","                    accruacy = 0\n","                    if tag_real == tag_pred:\n","                        acc = 1\n","\n","                        # else if tags match but BIO tags are wrong, increment by half\n","                    elif tag_real.split('-')[-1] == tag_pred.split('-')[-1]:\n","                        acc = 0.5\n","\n","                    #mainly for debugging, check to see which entries cause an error\n","                    try:\n","                      measurements[tag_real.split('-')[-1]][tag_pred.split('-')[-1]] += acc\n","                      measurements[tag_real.split('-')[-1]]['Total'] += 1\n","                      measurements['Total'][tag_pred.split('-')[-1]] += 1\n","                      measurements['Total']['Total'] += 1\n","                    except:\n","                      #print(tag_real.split('-')[-1],tag_pred.split('-')[-1])\n","                      print(line_num, line)\n","\n","    #actual accuracy\n","    print('{0:.2f}%'.format((\n","                              measurements['Act']['Act'] +\n","                              measurements['Item']['Item'] +\n","                              measurements['Stat']['Stat'] +\n","                              measurements['O']['O']\n","                             ) / measurements['Total']['Total'] * 100))\n","    #accuracy using Yiyang's method\n","    print('{0:.2f}%'.format((\n","                              measurements['Act']['Act'] +\n","                              measurements['Item']['Item'] +\n","                              measurements['Stat']['Stat']\n","                             ) / (measurements['Total']['Total']-measurements['O']['O']) * 100))\n","\n","    print('Precision | Recall')\n","    print('{0:.3f} | {1:.3f}'.format(measurements['Act']['Act'] / measurements['Total']['Act'],\n","                                     measurements['Act']['Act'] / measurements['Act']['Total']))\n","    print('{0:.3f} | {1:.3f}'.format(measurements['Item']['Item'] / measurements['Total']['Item'],\n","                                     measurements['Item']['Item'] / measurements['Item']['Total']))\n","    print('{0:.3f} | {1:.3f}'.format(measurements['Stat']['Stat'] / measurements['Total']['Stat'],\n","                                     measurements['Stat']['Stat'] / measurements['Stat']['Total']))\n","    print('{0:.3f} | {1:.3f}'.format(measurements['O']['O'] / measurements['Total']['O'],\n","                                     measurements['O']['O'] / measurements['O']['Total']))\n","\n","else:\n","    taggedData = predictor.predict(\"Repaired the engine fan\")['tag_logits']\n","    tag_ids = np.argmax(taggedData, axis=-1)\n","    print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n","\n","end = time.time()\n","#print(end - start)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["60.68%\n","42.82%\n","254 505 169 580 \n","\n","dict_values([175, 36.5, 0, 40.5, 254])\n","dict_values([15.0, 199.5, 1, 252.5, 505])\n","dict_values([2, 23.0, 69.5, 71.5, 169])\n","dict_values([9.5, 96.0, 2, 471, 580])\n","dict_values([203, 382, 73, 850, 1508])\n","Precision | Recall\n","0.862 | 0.689\n","0.522 | 0.395\n","0.952 | 0.411\n","0.554 | 0.812\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dugmrs0uKZJ9","colab_type":"code","colab":{}},"source":["#convert output into a form that neo4j can read\n","\n","file = 'drive/My Drive/Colab Notebooks/dat/Prediction.txt'\n","output_file = 'drive/My Drive/Colab Notebooks/dat/neo4jTriples.csv'\n","import csv\n","\n","with open(file, 'r') as f:\n","    entities_list = []\n","    for line in f:\n","        num, line = line.split(\":\",1) \n","        l = line.strip().split(' ')\n","        tag_prev = \"O\"\n","        entity_count = -1\n","        entities = []\n","        for pair in l:\n","            [w,t] = pair.split('~')\n","            if t == \"O\":\n","                tag_prev = \"O\"\n","            elif t.split('-')[-1] == tag_prev:\n","                entities[entity_count][1]= entities[entity_count][1] + ' ' + w\n","            else:\n","                entity_count +=1\n","                tag_prev = t.split('-')[-1]\n","                entities.append([num,w,tag_prev])\n","        entities_list.append(entities)\n","\n","with open(output_file, \"w\", newline='') as o:\n","    w = csv.writer(o)\n","    for entry in entities_list:\n","        for entity in entry:\n","            if entity[2] == 'Item' and entity[1] != '':\n","                for desc in entry:\n","                    if desc[2] == 'Act' or desc[2] == 'Stat':\n","                        w.writerow([entity[0],entity[1],desc[2],desc[1]])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrVN8xAzkTZB","colab_type":"code","outputId":"3ceb309f-3dd9-4a9a-e1eb-1c033715acd2","executionInfo":{"status":"ok","timestamp":1571199742913,"user_tz":-480,"elapsed":2245,"user":{"displayName":"Bryan Trac","photoUrl":"","userId":"17405550260864866588"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#link back to the raw data and join up\n","import csv\n","input_file = 'drive/My Drive/Colab Notebooks/dat/neo4jTriples.csv'\n","output_file = 'drive/My Drive/Colab Notebooks/dat/neo4jTriples-detailed.csv'\n","csv_file = 'drive/My Drive/Colab Notebooks/dat/Loader Work Orders unsanitised.csv'\n","clean_file = 'drive/My Drive/Colab Notebooks/dat/output-maintenance-item.csv'\n","\n","shorttext = []\n","train_size = 4000\n","test_size = 20000\n","size = train_size + test_size\n","count = 0\n","offset = 2\n","\n","with open(csv_file, newline='') as csvfile:\n","    for num, line in enumerate(csvfile.readlines()):\n","        array = line.split('|')\n","        shorttext.append(array)     \n","        #shorttext[count]\n","        #count = count+1\n","        if (num>=size):\n","          break\n","\n","with open(clean_file, newline='') as c:\n","    for num, line in enumerate(c.readlines()):\n","        line = line.replace('=',' ')\n","        line = line.replace('~',' ')\n","        line = line.replace('#',' ')\n","        line = line.replace('  ',' ')\n","        array = line.strip().split(',')\n","        shorttext[num].append(array[1])\n","        if (num>=size):\n","          break\n","          \n","print(shorttext[10])\n","with open(input_file, newline='') as input, open(output_file, \"w\", newline='') as o:\n","  w = csv.writer(o)\n","  for num,line in enumerate(input):\n","    data = line.strip().split(\",\")\n","    linkdata = shorttext[int(data[0])+offset]\n","    data.extend([linkdata[2],linkdata[6],linkdata[7],linkdata[4],linkdata[20]])\n","    #adds date, functional location, and raw shorttext for extra validation\n","    w.writerow(data)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['PM01', '100001661', '27/07/2005', '28/07/2005', 'Change out Main Gearbox oil To 85/140', 'MM21', 'EDD0064-B-150-01', 'Oil - Pump Drive Gearbox', 'CNCI PLND SCHD CHKD', 'CLSD PCNF PRT  MANC NMAT PRC  SETC', '72.34', '5005', '1', '143', 'FALSE', 'EDD0064', 'TRUE', '', 'FALSE', '\\r\\n', 'changeout main gearbox oil to _number_']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hsTr26-Uu37k","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}